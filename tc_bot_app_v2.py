import streamlit as st
import os
import zipfile
import tempfile
import pandas as pd
import requests
import re

# âœ… OpenRouter API Key (ë³´ì•ˆì„ ìœ„í•´ secrets.toml ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© ê¶Œì¥)
API_KEY = st.secrets.get("OPENROUTER_API_KEY") or os.environ.get(
    "OPENROUTER_API_KEY")

if not API_KEY:
    st.warning(
        "âš ï¸ OpenRouter API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .streamlit/secrets.tomlì— OPENROUTER_API_KEY í•­ëª©ì„ ì¶”ê°€í•˜ì„¸ìš”."
    )

st.set_page_config(page_title="ğŸ§  TC-Bot: QA ìë™í™” ë„ìš°ë¯¸", layout="wide")
st.title("ğŸ¤– TC-Bot: AI ê¸°ë°˜ QA ìë™í™” ë„ìš°ë¯¸")

# âœ… ì„¸ì…˜ ì´ˆê¸°í™” (íƒ­ ì„ ì–¸ë³´ë‹¤ ë¨¼ì € ìˆ˜í–‰í•´ì•¼ í•¨)
for key in ["scenario_result", "spec_result", "llm_result", "parsed_df", "last_uploaded_file", "last_model", "last_role", "is_loading"]:
    if key not in st.session_state:
        st.session_state[key] = None

if st.session_state["is_loading"] is None:
    st.session_state["is_loading"] = False


# âœ… ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    model = st.selectbox("ğŸ¤– ì‚¬ìš©í•  LLM ëª¨ë¸", ["qwen/qwen-max", "mistral"])
    qa_role = st.selectbox("ğŸ‘¤ QA ì—­í• ", ["ê¸°ëŠ¥ QA", "ë³´ì•ˆ QA", "ì„±ëŠ¥ QA"])
    st.session_state["qa_role"] = qa_role

# âœ… íƒ­ êµ¬ì„±
code_tab , tc_tab, log_tab = st.tabs(
    ["ğŸ§ª ì†ŒìŠ¤ì½”ë“œ â†’ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìë™ ìƒì„±","ğŸ“‘ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ â†’ ëª…ì„¸ì„œ ìš”ì•½","ğŸ ì—ëŸ¬ ë¡œê·¸ â†’ ì¬í˜„ ì‹œë‚˜ë¦¬ì˜¤" ])

# âœ… LLM í˜¸ì¶œ ì¤‘ ê²½ê³  í‘œì‹œ (íƒ­ ì°¨ë‹¨í•˜ì§€ ì•ŠìŒ)
if st.session_state["is_loading"]:
    st.warning("âš ï¸ í˜„ì¬ LLM í˜¸ì¶œ ì¤‘ì…ë‹ˆë‹¤. íƒ­ ì´ë™ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ë‹¤ë¥¸ ìš”ì²­ì€ ì™„ë£Œ í›„ ì‹œë„í•´ ì£¼ì„¸ìš”.")
else:
    st.empty()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”§ ìœ í‹¸ í•¨ìˆ˜: ì—ëŸ¬ ë¡œê·¸ ì „ì²˜ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_TOKEN_LIMITS = {
    "qwen/qwen-max": 30720,
    "mistral": 8192,
}


def safe_char_budget(model: str, token_margin: int = 1024) -> int:
    limit_tokens = MODEL_TOKEN_LIMITS.get(model, 8192)
    usable_tokens = max(1024, limit_tokens - token_margin)
    return usable_tokens * 4


def preprocess_log_text(text: str,
                        context_lines: int = 3,
                        keep_last_lines_if_empty: int = 1500,
                        char_budget: int = 120000) -> tuple[str, dict]:
    lines = text.splitlines()
    total_lines = len(lines)
    non_debug = [(i, line) for i, line in enumerate(lines)
                 if "DEBUG" not in line]
    patt = re.compile(r"(ERROR|Exception|WARN|FATAL)", re.IGNORECASE)
    matched_indices = [i for i, line in non_debug if patt.search(line)]
    selected = set()
    if matched_indices:
        for mi in matched_indices:
            orig_idx = non_debug[mi][0]
            for j in range(max(0, orig_idx - context_lines),
                           min(total_lines, orig_idx + context_lines + 1)):
                selected.add(j)
        focused = [lines[j] for j in sorted(selected)]
        header = [
            "### Log Focus (ERROR/WARN/Exception ì¤‘ì‹¬ ë°œì·Œ)",
            f"- ì „ì²´ ë¼ì¸: {total_lines:,}", f"- ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ë¼ì¸: {len(selected):,}", ""
        ]
        trimmed = "\n".join(header + focused)
    else:
        tail = lines[-keep_last_lines_if_empty:]
        header = [
            "### Log Tail (ë§¤ì¹˜ ì—†ìŒ â†’ ë§ˆì§€ë§‰ ì¼ë¶€ ì‚¬ìš©)", f"- ì „ì²´ ë¼ì¸: {total_lines:,}",
            f"- ì‚¬ìš© ë¼ì¸(ë§ˆì§€ë§‰): {len(tail):,}", ""
        ]
        trimmed = "\n".join(header + tail)
    if len(trimmed) > char_budget:
        trimmed = trimmed[-char_budget:]
    stats = {
        "total_lines": total_lines,
        "kept_chars": len(trimmed),
        "char_budget": char_budget
    }
    return trimmed, stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§ª TAB 1: ì†ŒìŠ¤ì½”ë“œ â†’ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìë™ ìƒì„±ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with code_tab:
    st.subheader("ğŸ§ª ì†ŒìŠ¤ì½”ë“œ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìë™ ìƒì„±ê¸°")
    uploaded_file = st.file_uploader("ğŸ“‚ ì†ŒìŠ¤ì½”ë“œ zip íŒŒì¼ ì—…ë¡œë“œ",
                                     type=["zip"],
                                     key="code_zip")

    def need_llm_call(uploaded_file, model, role):
        return uploaded_file and (st.session_state.last_uploaded_file
                                  != uploaded_file.name
                                  or st.session_state.last_model != model
                                  or st.session_state.last_role != role)

    qa_role = st.session_state.get("qa_role", "ê¸°ëŠ¥ QA")

    if uploaded_file and need_llm_call(uploaded_file, model, qa_role):
        st.session_state["is_loading"] = True
        with st.spinner("ğŸ” LLM í˜¸ì¶œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."):
            with tempfile.TemporaryDirectory() as tmpdir:
                zip_path = os.path.join(tmpdir, uploaded_file.name)
                with open(zip_path, "wb") as f:
                    f.write(uploaded_file.read())
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(tmpdir)
                full_code = ""
                for root, _, files in os.walk(tmpdir):
                    for file in files:
                        if file.endswith((".py", ".java", ".js", ".ts", ".cpp",
                                          ".c", ".cs")):
                            file_path = os.path.join(root, file)
                            try:
                                with open(file_path,
                                          "r",
                                          encoding="utf-8",
                                          errors="ignore") as f:
                                    code = f.read()
                                    full_code += f"\n\n# FILE: {file}\n{code}"
                            except:
                                continue
            prompt = f"""
ë„ˆëŠ” ì‹œë‹ˆì–´ QA ì—”ì§€ë‹ˆì–´ì´ë©°, í˜„ì¬ '{qa_role}' ì—­í• ì„ ë§¡ê³  ìˆë‹¤.
ì•„ë˜ì— ì œê³µëœ ì†ŒìŠ¤ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ê¸°ëŠ¥ ë‹¨ìœ„ì˜ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ìƒì„±í•˜ë¼.

ğŸ“Œ ì¶œë ¥ í˜•ì‹ì€ ì•„ë˜ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•íƒœë¡œ ì‘ì„±í•˜ë˜,
ìš°ì„ ìˆœìœ„ëŠ” ë°˜ë“œì‹œ High / Medium / Low ì¤‘ í•˜ë‚˜ë¡œ ì‘ì„±í•  ê²ƒ:

| TC ID | ê¸°ëŠ¥ ì„¤ëª… | ì…ë ¥ê°’ | ì˜ˆìƒ ê²°ê³¼ | ìš°ì„ ìˆœìœ„ |
|-------|-----------|--------|------------|---------|

ì†ŒìŠ¤ì½”ë“œ:
{full_code}
"""
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={"Authorization": f"Bearer {API_KEY}"},
                json={
                    "model": model,
                    "messages": [{
                        "role": "user",
                        "content": prompt
                    }]
                })
            result = response.json()["choices"][0]["message"]["content"]
            st.session_state.llm_result = result
            rows = []
            for line in result.splitlines():
                if "|" in line and "TC" in line:
                    parts = [p.strip() for p in line.strip().split("|")[1:-1]]
                    if len(parts) == 5:
                        rows.append(parts)
            if rows:
                df = pd.DataFrame(
                    rows, columns=["TC ID", "ê¸°ëŠ¥ ì„¤ëª…", "ì…ë ¥ê°’", "ì˜ˆìƒ ê²°ê³¼", "ìš°ì„ ìˆœìœ„"])
                st.session_state.parsed_df = df
            st.session_state.last_uploaded_file = uploaded_file.name
            st.session_state.last_model = model
            st.session_state.last_role = qa_role
        st.session_state["is_loading"] = False

    if st.session_state.llm_result:
        st.success("âœ… í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
        st.markdown("## ğŸ“‹ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤")
        st.markdown(st.session_state.llm_result)

    if st.session_state.parsed_df is not None and not need_llm_call(
            uploaded_file, model, qa_role):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
            st.session_state.parsed_df.to_excel(tmp.name, index=False)
            tmp.seek(0)
            st.download_button("â¬‡ï¸ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                               data=tmp.read(),
                               file_name="í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤.xlsx")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“‘ TAB 2: í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ â†’ ëª…ì„¸ì„œ ìš”ì•½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tc_tab:
    st.subheader("ğŸ“‘ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ê¸°ë°˜ ê¸°ëŠ¥/ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ì¶”ì¶œê¸°")
    tc_file = st.file_uploader("ğŸ“‚ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ íŒŒì¼ ì—…ë¡œë“œ (.xlsx, .csv)",
                               type=["xlsx", "csv"],
                               key="tc_file")
    summary_type = st.selectbox("ğŸ“Œ ìš”ì•½ ìœ í˜•", ["ê¸°ëŠ¥ ëª…ì„¸ì„œ", "ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œ"],
                                key="summary_type")
    if st.button("ğŸš€ ëª…ì„¸ì„œ ìƒì„±í•˜ê¸°", disabled=st.session_state["is_loading"]) and tc_file:
        st.session_state["is_loading"] = True
        try:
            if tc_file.name.endswith("csv"):
                df = pd.read_csv(tc_file)
            else:
                df = pd.read_excel(tc_file)
        except Exception as e:
            st.session_state["is_loading"] = False
            st.error(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            st.stop()
        required_cols = ["TC ID", "ê¸°ëŠ¥ ì„¤ëª…", "ì…ë ¥ê°’", "ì˜ˆìƒ ê²°ê³¼"]
        if not all(col in df.columns for col in required_cols):
            st.session_state["is_loading"] = False
            st.warning("âš ï¸ ë‹¤ìŒ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤: TC ID, ê¸°ëŠ¥ ì„¤ëª…, ì…ë ¥ê°’, ì˜ˆìƒ ê²°ê³¼")
            st.stop()
        prompt = f"""
ë„ˆëŠ” í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ê·¸ ê¸°ë°˜ì´ ë˜ëŠ” {summary_type}ë¥¼ ì‘ì„±í•˜ëŠ” QA ì „ë¬¸ê°€ì´ë‹¤.
ë‹¤ìŒ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ ê¸°ëŠ¥ëª… ë˜ëŠ” ìš”êµ¬ì‚¬í•­ ì œëª©ê³¼ í•¨ê»˜, ì„¤ëª…ê³¼ ëª©ì ì„ ìì—°ì–´ë¡œ ìš”ì•½í•˜ë¼.

í˜•ì‹:
- ê¸°ëŠ¥ëª… ë˜ëŠ” ìš”êµ¬ì‚¬í•­ ì œëª©
- ì„¤ëª…
- ê¸°ëŒ€ íš¨ê³¼

í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ëª©ë¡:
{df.to_csv(index=False)}
"""
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={"Authorization": f"Bearer {API_KEY}"},
            json={
                "model": model,
                "messages": [{
                    "role": "user",
                    "content": prompt
                }]
            })
        if response.status_code == 200:
            result = response.json()["choices"][0]["message"]["content"]
            st.session_state.spec_result = result
        else:
            st.error("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨")
            st.text(response.text)
        st.session_state["is_loading"] = False

    if st.session_state.spec_result:
        st.success("âœ… ëª…ì„¸ì„œ ìƒì„± ì™„ë£Œ!")
        st.markdown("## ğŸ“‹ ìë™ ìƒì„±ëœ ëª…ì„¸ì„œ")
        st.markdown(st.session_state.spec_result)
        st.download_button("â¬‡ï¸ ëª…ì„¸ì„œ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                           data=st.session_state.spec_result,
                           file_name="ê¸°ëŠ¥_ìš”êµ¬ì‚¬í•­_ëª…ì„¸ì„œ.txt")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ TAB 3: ì—ëŸ¬ ë¡œê·¸ â†’ ì¬í˜„ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with log_tab:
    st.subheader("ğŸ ì—ëŸ¬ ë¡œê·¸ ê¸°ë°˜ ì¬í˜„ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ê¸°")

    # âœ… ìƒ˜í”Œ ì—ëŸ¬ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€
    sample_log = """[InstallShield Silent]
    Version=v7.00
    File=Log File
    [ResponseResult]
    ResultCode=0
    [Application]
    Name=Realtek Audio Driver
    Version=4.92
    Company=Realtek Semiconductor Corp.
    Lang=0412
    """

    st.download_button(
    "â¬‡ï¸ ìƒ˜í”Œ ì—ëŸ¬ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ",
    data=sample_log,
    file_name="sample_error_log.log",
    disabled=st.session_state["is_loading"]
)

    log_file = st.file_uploader("ğŸ“‚ ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ì—…ë¡œë“œ (.log, .txt)",
                                type=["log", "txt"],
                                key="log_file")
    if not API_KEY:
        st.warning("ğŸ” OpenRouter API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if st.button("ğŸš€ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±í•˜ê¸°", disabled=st.session_state["is_loading"]) and log_file:
        st.session_state["is_loading"] = True
        with st.spinner("LLMì„ í˜¸ì¶œ ì¤‘ì…ë‹ˆë‹¤..."):
            raw_log = log_file.read().decode("utf-8", errors="ignore")
            qa_role = st.session_state.get("qa_role", "ê¸°ëŠ¥ QA")
            chosen_model = model
            budget = safe_char_budget(chosen_model, token_margin=1024)
            focused_log, stats = preprocess_log_text(
                raw_log,
                context_lines=5,
                keep_last_lines_if_empty=2000,
                char_budget=budget)
            st.info(
                f"ì „ì²˜ë¦¬ ê²°ê³¼: ë¬¸ì {stats['kept_chars']:,}/{stats['char_budget']:,} ì‚¬ìš© (ì „ì²´ ë¼ì¸ {stats['total_lines']:,})."
            )
            prompt = f"""ë„ˆëŠ” ì‹œë‹ˆì–´ QA ì—”ì§€ë‹ˆì–´ì´ë©°, í˜„ì¬ '{qa_role}' ì—­í• ì„ ë§¡ê³  ìˆë‹¤.
ì•„ë˜ ìš”ì•½Â·ë°œì·Œí•œ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ í•´ë‹¹ ì˜¤ë¥˜ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆëŠ” í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‘ì„±í•˜ë¼.

ì‹œë‚˜ë¦¬ì˜¤ í˜•ì‹:
1. ì‹œë‚˜ë¦¬ì˜¤ ì œëª©:
2. ì „ì œ ì¡°ê±´:
3. í…ŒìŠ¤íŠ¸ ì…ë ¥ê°’:
4. ì¬í˜„ ì ˆì°¨:
5. ê¸°ëŒ€ ê²°ê³¼:

ì „ì²˜ë¦¬ëœ ì—ëŸ¬ ë¡œê·¸:
{focused_log}
"""
            try:
                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={"Authorization": f"Bearer {API_KEY}"},
                    json={
                        "model": chosen_model,
                        "messages": [{
                            "role": "user",
                            "content": prompt
                        }],
                        "temperature": 0.2
                    },
                    timeout=120,
                )
                if response.status_code == 200:
                    content = response.json(
                    )["choices"][0]["message"]["content"]
                    st.session_state.scenario_result = content
                else:
                    st.error("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨")
                    st.caption("ì„œë²„ ì‘ë‹µ:")
                    st.text(response.text)
            except requests.exceptions.RequestException as e:
                st.error("âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ")
                st.exception(e)
        st.session_state["is_loading"] = False
    if st.session_state.scenario_result:
        st.success("âœ… ì¬í˜„ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì™„ë£Œ!")
        st.markdown("## ğŸ“‹ ìë™ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤")
        st.markdown(st.session_state.scenario_result)
        st.download_button("â¬‡ï¸ ì‹œë‚˜ë¦¬ì˜¤ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                           data=st.session_state.scenario_result,
                           file_name="ì¬í˜„_ì‹œë‚˜ë¦¬ì˜¤.txt")




