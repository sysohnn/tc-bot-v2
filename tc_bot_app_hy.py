import streamlit as st
import os
import zipfile
import tempfile
import pandas as pd
import requests
import re

# [ADD] ìœ í‹¸/ë¯¸ë¦¬ë³´ê¸°/ì—‘ì…€ìš©
import io
from collections import Counter, defaultdict
from hashlib import sha1
from pathlib import Path

# âœ… OpenRouter API Key (ë³´ì•ˆì„ ìœ„í•´ secrets.toml ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© ê¶Œì¥)
API_KEY = st.secrets.get("OPENROUTER_API_KEY") or os.environ.get(
    "OPENROUTER_API_KEY"
)
if not API_KEY:
    st.warning(
        "âš ï¸ OpenRouter API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .streamlit/secrets.tomlì— OPENROUTER_API_KEY í•­ëª©ì„ ì¶”ê°€í•˜ì„¸ìš”."
    )

st.set_page_config(page_title="ğŸ§  TC-Bot: QA ìë™í™” ë„ìš°ë¯¸", layout="wide")
st.title("ğŸ¤– TC-Bot: AI ê¸°ë°˜ QA ìë™í™” ë„ìš°ë¯¸")

# âœ… ì„¸ì…˜ ì´ˆê¸°í™” (íƒ­ ì„ ì–¸ë³´ë‹¤ ë¨¼ì € ìˆ˜í–‰í•´ì•¼ í•¨)
for key in ["scenario_result", "spec_result", "llm_result", "parsed_df", "last_uploaded_file", "last_model", "last_role", "is_loading"]:
    if key not in st.session_state:
        st.session_state[key] = None

# [ADD] ê¸°ëŠ¥ë³„ ê·¸ë£¹ ë³´ê´€ + ì •ê·œí™” ì›ë¬¸ ë³´ê´€ + ê¸°ëŠ¥íŒíŠ¸ ë³´ê´€
if "parsed_groups" not in st.session_state:
    st.session_state["parsed_groups"] = None
if "normalized_markdown" not in st.session_state:
    st.session_state["normalized_markdown"] = None
# [ADD] ì—…ë¡œë“œ ì½”ë“œì—ì„œ ì¶”ì¶œí•œ ê¸°ëŠ¥ íŒíŠ¸ ì €ì¥ (í›„ì²˜ë¦¬ ë¶„ë¦¬ì— í™œìš©)
if "feature_hints" not in st.session_state:
    st.session_state["feature_hints"] = None
if st.session_state["is_loading"] is None:
    st.session_state["is_loading"] = False

# âœ… ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    model = st.selectbox("ğŸ¤– ì‚¬ìš©í•  LLM ëª¨ë¸", ["qwen/qwen-max", "mistral"])
    qa_role = st.selectbox("ğŸ‘¤ QA ì—­í• ", ["ê¸°ëŠ¥ QA", "ë³´ì•ˆ QA", "ì„±ëŠ¥ QA"])
    st.session_state["qa_role"] = qa_role

# âœ… íƒ­ êµ¬ì„±
code_tab , tc_tab, log_tab = st.tabs(
    ["ğŸ§ª ì†ŒìŠ¤ì½”ë“œ â†’ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìë™ ìƒì„±","ğŸ“‘ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ â†’ ëª…ì„¸ì„œ ìš”ì•½","ğŸ ì—ëŸ¬ ë¡œê·¸ â†’ ì¬í˜„ ì‹œë‚˜ë¦¬ì˜¤"]
)

# âœ… LLM í˜¸ì¶œ ì¤‘ ê²½ê³  í‘œì‹œ (íƒ­ ì°¨ë‹¨í•˜ì§€ ì•ŠìŒ)
if st.session_state["is_loading"]:
    st.warning("âš ï¸ í˜„ì¬ LLM í˜¸ì¶œ ì¤‘ì…ë‹ˆë‹¤. íƒ­ ì´ë™ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ë‹¤ë¥¸ ìš”ì²­ì€ ì™„ë£Œ í›„ ì‹œë„í•´ ì£¼ì„¸ìš”.")
else:
    st.empty()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”§ ìœ í‹¸ í•¨ìˆ˜: ì—ëŸ¬ ë¡œê·¸ ì „ì²˜ë¦¬ (ê¸°ì¡´ ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_TOKEN_LIMITS = {
    "qwen/qwen-max": 30720,
    "mistral": 8192,
}
def safe_char_budget(model: str, token_margin: int = 1024) -> int:
    limit_tokens = MODEL_TOKEN_LIMITS.get(model, 8192)
    usable_tokens = max(1024, limit_tokens - token_margin)
    return usable_tokens * 4

def preprocess_log_text(text: str, context_lines: int = 3, keep_last_lines_if_empty: int = 1500, char_budget: int = 120000) -> tuple[str, dict]:
    lines = text.splitlines()
    total_lines = len(lines)
    non_debug = [(i, line) for i, line in enumerate(lines) if "DEBUG" not in line]
    patt = re.compile(r"(ERROR|Exception|WARN|FATAL)", re.IGNORECASE)
    matched_indices = [i for i, line in non_debug if patt.search(line)]

    selected = set()
    if matched_indices:
        for mi in matched_indices:
            orig_idx = non_debug[mi][0]
            for j in range(max(0, orig_idx - context_lines), min(total_lines, orig_idx + context_lines + 1)):
                selected.add(j)
        focused = [lines[j] for j in sorted(selected)]
        header = [
            "### Log Focus (ERROR/WARN/Exception ì¤‘ì‹¬ ë°œì·Œ)",
            f"- ì „ì²´ ë¼ì¸: {total_lines:,}",
            f"- ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ë¼ì¸: {len(selected):,}",
            ""
        ]
        trimmed = "\n".join(header + focused)
    else:
        tail = lines[-keep_last_lines_if_empty:]
        header = [
            "### Log Tail (ë§¤ì¹˜ ì—†ìŒ â†’ ë§ˆì§€ë§‰ ì¼ë¶€ ì‚¬ìš©)",
            f"- ì „ì²´ ë¼ì¸: {total_lines:,}",
            f"- ì‚¬ìš© ë¼ì¸(ë§ˆì§€ë§‰): {len(tail):,}",
            ""
        ]
        trimmed = "\n".join(header + tail)

    if len(trimmed) > char_budget:
        trimmed = trimmed[-char_budget:]

    stats = {
        "total_lines": total_lines,
        "kept_chars": len(trimmed),
        "char_budget": char_budget
    }
    return trimmed, stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [ADD] ìƒ˜í”Œ íŒŒì¼/ìƒ˜í”Œ TC ì—‘ì…€ ë¹Œë” (ê¸°ì¡´ ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_sample_code_zip() -> bytes:
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr("app.py", "# FILE: app.py\n"
                    "def add(a, b):\n"
                    "    return a + b\n\n"
                    "def div(a, b):\n"
                    "    if b == 0:\n"
                    "        raise ZeroDivisionError('b must not be zero')\n"
                    "    return a / b\n")
        zf.writestr("utils/validator.py", "# FILE: utils/validator.py\n"
                    "def is_email(s: str) -> bool:\n"
                    "    return '@' in s and '.' in s.split('@')[-1]\n")
        zf.writestr("README.md", "# Sample Project\n\n"
                    "- add(a,b), div(a,b), is_email(s) í•¨ìˆ˜ í¬í•¨\n"
                    "- ë‹¨ìˆœ ì‚°ìˆ /ê²€ì¦ ë¡œì§ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„± ì‹œì—°ìš©")
    return buf.getvalue()

def build_sample_tc_excel() -> bytes:
    df = pd.DataFrame([
        ["TC-001", "ë§ì…ˆ ê¸°ëŠ¥", "a=1, b=2", "3 ë°˜í™˜", "High"],
        ["TC-002", "ë‚˜ëˆ—ì…ˆ ê¸°ëŠ¥(ì •ìƒ)", "a=6, b=3", "2 ë°˜í™˜", "Medium"],
        ["TC-003", "ë‚˜ëˆ—ì…ˆ ê¸°ëŠ¥(ì˜ˆì™¸)", "a=1, b=0", "ZeroDivisionError ë°œìƒ", "High"],
        ["TC-004", "ì´ë©”ì¼ ê²€ì¦(ì •ìƒ)", "s='user@example.com'", "True ë°˜í™˜", "Low"],
        ["TC-005", "ì´ë©”ì¼ ê²€ì¦(ì´ìƒ)", "s='invalid@domain'", "False ë˜ëŠ” ê·œì¹™ ìœ„ë°˜ ì²˜ë¦¬", "Low"],
    ], columns=["TC ID", "ê¸°ëŠ¥ ì„¤ëª…", "ì…ë ¥ê°’", "ì˜ˆìƒ ê²°ê³¼", "ìš°ì„ ìˆœìœ„"])
    bio = io.BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤")
    return bio.getvalue()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [ADD] ì½”ë“œ ZIP ë¶„ì„/í”„ë¦¬ë·° ìœ í‹¸ (ê¸°ì¡´ ë¡œì§ í™•ì¥: í´ë˜ìŠ¤/íŒŒì¼/ë””ë ‰í„°ë¦¬ â†’ ê¸°ëŠ¥íŒíŠ¸ ì¶”ì¶œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _norm_key(s: str) -> str:
    s = re.sub(r"[^\w\-]+", " ", s)
    s = re.sub(r"[_\s]+", "-", s).strip("-").lower()
    return s or "general"

def _display_from_key(key: str) -> str:
    parts = [p for p in key.split("-") if p]
    return "".join(w.capitalize() for w in parts) or "General"

def analyze_code_zip(zip_bytes: bytes) -> dict:
    lang_map = {
        ".py": "Python",
        ".java": "Java",
        ".js": "JS",
        ".ts": "TS",
        ".cpp": "CPP",
        ".c": "C",
        ".cs": "CS"
    }
    lang_counts = Counter()
    top_functions = []
    classes = []
    total_files = 0
    module_counts = Counter()
    sample_paths = []
    feature_keys = set()  # [ADD] ê¸°ëŠ¥ í›„ë³´ í‚¤

    try:
        with zipfile.ZipFile(io.BytesIO(zip_bytes), "r") as zf:
            names = zf.namelist()
            total_files = len(names)
            sample_paths = names[:10]

            for n in names:
                if n.endswith("/"):
                    # ìƒìœ„ ë””ë ‰í„°ë¦¬ëª…ì„ ê¸°ëŠ¥ í›„ë³´ë¡œ
                    first = n.strip("/").split("/")[0]
                    if first:
                        feature_keys.add(_norm_key(first))
                    continue
                parts = n.split("/")
                module = parts[0] if len(parts) > 1 else "(root)"
                module_counts[module] += 1

                ext = os.path.splitext(n)[1].lower()
                stem = os.path.splitext(os.path.basename(n))[0]
                if stem:
                    feature_keys.add(_norm_key(stem))  # íŒŒì¼ëª…ë„ í›„ë³´

                if ext in lang_map:
                    lang_counts[lang_map[ext]] += 1

                try:
                    with zf.open(n) as fh:
                        content = fh.read(100_000).decode("utf-8", errors="ignore")
                        # í•¨ìˆ˜/ë©”ì„œë“œ
                        for pat in [
                            r"def\s+([a-zA-Z_]\w*)\s*\(",
                            r"function\s+([a-zA-Z_]\w*)\s*\(",
                            r"(?:public|private|protected)?\s*(?:static\s+)?[A-Za-z_<>\[\]]+\s+([a-zA-Z_]\w*)\s*\("
                        ]:
                            top_functions += re.findall(pat, content)
                        # í´ë˜ìŠ¤
                        for cpat in [
                            r"class\s+([A-Z][A-Za-z0-9_]*)",
                            r"(?:public|final|abstract)\s+class\s+([A-Z][A-Za-z0-9_]*)"
                        ]:
                            classes += re.findall(cpat, content)
                except Exception:
                    pass

        # í´ë˜ìŠ¤/í•¨ìˆ˜ëª…ë„ ê¸°ëŠ¥ í›„ë³´
        for name in classes[:80]:
            feature_keys.add(_norm_key(name))
        for fn in top_functions[:120]:
            feature_keys.add(_norm_key(fn))

    except zipfile.BadZipFile:
        pass

    # ë„ˆë¬´ ì¼ë°˜ì ì¸ í‚¤ ì œê±°
    generic = {"app","main","index","core","utils","common","service","controller","model","routes","handler","api","src","test","tests"}
    feature_keys = {k for k in feature_keys if k and k not in generic}

    return {
        "total_files": total_files,
        "lang_counts": lang_counts,
        "top_functions": top_functions[:200],
        "module_counts": module_counts,
        "sample_paths": sample_paths,
        # [ADD]
        "classes": classes[:200],
        "feature_keys": sorted(feature_keys)[:40],  # í”„ë¡¬í”„íŠ¸ ë¶€ë‹´ ì™„í™”
    }

def estimate_tc_count(stats: dict) -> int:
    files = max(0, stats.get("total_files", 0))
    langs = sum(stats.get("lang_counts", Counter()).values())
    funcs = len(stats.get("top_functions", []))
    estimate = int(files * 0.3 + langs * 0.7 + funcs * 0.9)
    return max(3, min(estimate, 300))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [ADD] LLM ê²°ê³¼ íŒŒì‹±/ì •ê·œí™” ìœ í‹¸ í™•ì¥ (ê¸°ëŠ¥ íŒíŠ¸ ê¸°ë°˜ ê°•ì œ ë¶„ë¦¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _strip_code_fences(md: str) -> str:
    return re.sub(r"```.*?```", "", md, flags=re.DOTALL)

def _parse_md_tables_with_heading(md_text: str) -> list[tuple[str, pd.DataFrame]]:
    text = _strip_code_fences(md_text)
    lines = text.splitlines()
    tables = []
    i = 0
    last_heading = None
    heading_line = -999

    while i < len(lines):
        line = lines[i].rstrip()

        m = re.match(r"^\s{0,3}#{1,6}\s+(.+?)\s*$", line)
        if m:
            last_heading = m.group(1).strip()
            heading_line = i
            i += 1
            continue

        if "|" in line and i + 1 < len(lines) and re.search(r"\|\s*:?-{2,}\s*\|", lines[i + 1]):
            feature_name = last_heading if 0 <= (i - heading_line - 1) <= 3 else ""
            j = i + 2
            rows = [line, lines[i + 1]]
            while j < len(lines):
                cur = lines[j]
                if cur.strip() == "" or ("|" not in cur):
                    break
                rows.append(cur)
                j += 1
            df = _md_table_to_df("\n".join(rows))
            if df is not None and len(df.columns) >= 3:
                tables.append((feature_name, df))
            i = j
            continue

        i += 1

    return tables

def _md_table_to_df(table_str: str) -> pd.DataFrame | None:
    raw = [r for r in table_str.splitlines() if r.strip()]
    if len(raw) < 2:
        return None

    headers = [h.strip() for h in raw[0].strip("|").split("|")]
    data_lines = [r for r in raw[2:]]
    rows = []
    for line in data_lines:
        if "|" not in line:
            continue
        parts = [c.strip() for c in line.strip("|").split("|")]
        if len(parts) != len(headers):
            continue
        rows.append(parts)
    if not rows:
        return None
    return pd.DataFrame(rows, columns=headers)

def _normalize_headers(df: pd.DataFrame) -> pd.DataFrame:
    header_map = {
        "TC ID":"TC ID","TCID":"TC ID","ID":"TC ID","ì¼€ì´ìŠ¤ID":"TC ID",
        "ê¸°ëŠ¥ ì„¤ëª…":"ê¸°ëŠ¥ ì„¤ëª…","ê¸°ëŠ¥ì„¤ëª…":"ê¸°ëŠ¥ ì„¤ëª…","Feature":"ê¸°ëŠ¥ ì„¤ëª…","Description":"ê¸°ëŠ¥ ì„¤ëª…","ê¸°ëŠ¥":"ê¸°ëŠ¥ ì„¤ëª…",
        "ì…ë ¥ê°’":"ì…ë ¥ê°’","Input":"ì…ë ¥ê°’","ì…ë ¥":"ì…ë ¥ê°’","Parameters":"ì…ë ¥ê°’",
        "ì˜ˆìƒ ê²°ê³¼":"ì˜ˆìƒ ê²°ê³¼","Expected":"ì˜ˆìƒ ê²°ê³¼","Output":"ì˜ˆìƒ ê²°ê³¼","ê¸°ëŒ€ ê²°ê³¼":"ì˜ˆìƒ ê²°ê³¼","ê²°ê³¼":"ì˜ˆìƒ ê²°ê³¼",
        "ìš°ì„ ìˆœìœ„":"ìš°ì„ ìˆœìœ„","Priority":"ìš°ì„ ìˆœìœ„","ìš°ì„  ìˆœìœ„":"ìš°ì„ ìˆœìœ„"
    }
    new_cols = {}
    for c in df.columns:
        key = header_map.get(str(c).strip(), None)
        if key:
            new_cols[c] = key
    df2 = df.rename(columns=new_cols)
    for c in ["TC ID","ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼","ìš°ì„ ìˆœìœ„"]:
        if c not in df2.columns:
            df2[c] = ""
    return df2[["TC ID","ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼","ìš°ì„ ìˆœìœ„"]]

def _normalize_feature_key(name: str, sample_row: dict | None = None) -> tuple[str,str]:
    key = (name or "").strip()
    if not key and sample_row:
        tcid = str(sample_row.get("TC ID",""))
        feat = str(sample_row.get("ê¸°ëŠ¥ ì„¤ëª…",""))
        m = re.match(r"(?i)TC[-_]?([A-Za-z0-9]+)", tcid)
        if m and m.group(1) and not m.group(1).isdigit():
            key = m.group(1)
        if not key:
            tks = re.findall(r"[A-Za-z][A-Za-z0-9]+", feat)
            if tks:
                key = "".join(tks[:2])

    key = key or "General"
    sheet = re.sub(r"[^A-Za-z0-9ê°€-í£_ -]", "", key).strip()
    key_id = re.sub(r"[^A-Za-z0-9 ]", "", sheet).strip().lower().replace(" ", "-") or "general"
    return sheet, key_id

def _extract_prefix_from_tcid(tcid: str) -> str | None:
    m = re.match(r"(?i)^TC[-_]?([A-Za-z][A-Za-z0-9]+)-\d{2,4}$", str(tcid).strip())
    if m:
        return m.group(1).lower()
    return None

# [ADD] ê¸°ëŠ¥ íŒíŠ¸(aliases) ìƒì„±: ê° keyì— ëŒ€í•´ íŒŒì¼ëª…/í´ë˜ìŠ¤/í•¨ìˆ˜ íŒŒìƒ í† í° í¬í•¨
def build_feature_hints(stats: dict) -> dict:
    keys = stats.get("feature_keys", []) or []
    aliases = defaultdict(set)

    # ì› í‚¤
    for k in keys:
        aliases[k].add(k)
        aliases[k].add(k.replace("-", ""))

    # íŒŒì¼/í´ë˜ìŠ¤/í•¨ìˆ˜ì—ì„œ íŒŒìƒ í† í°
    for name in (stats.get("classes") or []) + (stats.get("top_functions") or []):
        norm = _norm_key(name)
        if not norm:
            continue
        # ê°€ì¥ ìœ ì‚¬í•œ í‚¤ì— ë§¤í•‘(ê°„ë‹¨: ì ‘ë‘ ì¼ì¹˜/ë¶€ë¶„ ì¼ì¹˜)
        target = None
        for k in keys:
            if norm.startswith(k) or k.startswith(norm) or norm.replace("-","") in k.replace("-",""):
                target = k; break
        if target:
            aliases[target].update({norm, norm.replace("-", ""), name.lower()})

    # ë””ë ‰í„°ë¦¬/íŒŒì¼ ê¸°ë°˜ í‚¤(ì´ë¯¸ analyzeì—ì„œ ë„£ì—ˆìŒ)
    return {k: sorted(v) for k, v in aliases.items()}

# [ADD] íŒíŠ¸ ê¸°ë°˜ í–‰â†’ê¸°ëŠ¥ í‚¤ ì¶”ì •
def _infer_key_from_row_with_hints(row: pd.Series, hints: dict) -> str:
    text = " ".join([str(row.get(c,"")) for c in ["TC ID","ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼"]]).lower()
    # TCID ì ‘ë‘ ìš°ì„ 
    pref = _extract_prefix_from_tcid(str(row.get("TC ID","")))
    if pref:
        return pref
    # íŒíŠ¸ í† í° ìŠ¤ìº”
    best_key, best_hits = None, 0
    for key, toks in hints.items():
        hits = 0
        for t in toks:
            t2 = t.lower()
            if t2 and t2 in text:
                hits += 1
        if hits > best_hits:
            best_key, best_hits = key, hits
    return best_key or "general"

def split_single_df_feature_aware(df: pd.DataFrame, hints: dict) -> dict[str, pd.DataFrame]:
    df2 = _normalize_headers(df).fillna("")
    df2["_k_"] = df2.apply(lambda r: _infer_key_from_row_with_hints(r, hints), axis=1)
    groups = {}
    for k, sub in df2.groupby("_k_"):
        sub = sub.drop(columns=["_k_"]).reset_index(drop=True)
        sheet, key_id = _normalize_feature_key(k, sub.iloc[0].to_dict() if len(sub) else None)
        sub["TC ID"] = [f"tc-{key_id}-{i:03d}" for i in range(1, len(sub)+1)]
        groups[sheet[:31] or "General"] = sub
    return groups

def group_tables_and_renumber(md_text: str) -> dict[str, pd.DataFrame]:
    tbls = _parse_md_tables_with_heading(md_text)
    if not tbls:
        return {}
    groups: dict[str, pd.DataFrame] = {}
    unnamed_count = 0

    for (heading, df) in tbls:
        df_norm = _normalize_headers(df).fillna("")
        sample_row = df_norm.iloc[0].to_dict() if len(df_norm) else {}
        sheet_name, key_id = _normalize_feature_key(heading, sample_row)
        if not heading:
            unnamed_count += 1
            sheet_name = f"{sheet_name}-{unnamed_count}"
        df_norm["TC ID"] = [f"tc-{key_id}-{i:03d}" for i in range(1, len(df_norm)+1)]
        final_name = sheet_name[:31] if len(sheet_name) > 31 else sheet_name
        cnt = 2
        while final_name in groups:
            candidate = (sheet_name[:27] + f"-{cnt}") if len(sheet_name) > 27 else f"{sheet_name}-{cnt}"
            final_name = candidate[:31]
            cnt += 1
        groups[final_name] = df_norm
    return groups

def concat_groups_for_view(groups: dict[str, pd.DataFrame]) -> pd.DataFrame:
    if not groups:
        return pd.DataFrame(columns=["ê¸°ëŠ¥","TC ID","ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼","ìš°ì„ ìˆœìœ„"])
    view_rows = []
    for sheet, df in groups.items():
        df2 = df.copy()
        df2["ê¸°ëŠ¥"] = sheet
        view_rows.append(df2)
    return pd.concat(view_rows, ignore_index=True)[["ê¸°ëŠ¥","TC ID","ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼","ìš°ì„ ìˆœìœ„"]]

def _df_to_md_table(df: pd.DataFrame) -> str:
    cols = ["TC ID","ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼","ìš°ì„ ìˆœìœ„"]
    use_cols = [c for c in cols if c in df.columns]
    header = "| " + " | ".join(use_cols) + " |"
    sep = "| " + " | ".join(["---"] * len(use_cols)) + " |"
    rows = []
    for _, r in df[use_cols].iterrows():
        rows.append("| " + " | ".join(str(r[c]) for c in use_cols) + " |")
    return "\n".join([header, sep] + rows)

# [FIX] í•µì‹¬: ì›ë¬¸ì„ ê¸°ëŠ¥ë³„ + IDì •ê·œí™” â€˜ì›ë¬¸í˜•ì‹â€™ìœ¼ë¡œ ì¬êµ¬ì„± (íŒíŠ¸ ê¸°ë°˜ ê°•ì œ ë¶„ë¦¬ í¬í•¨)
def rebuild_normalized_markdown(md_text: str, feature_hints: dict | None) -> tuple[str, dict[str, pd.DataFrame]]:
    groups = group_tables_and_renumber(md_text)
    if not groups:
        tbls = _parse_md_tables_with_heading(md_text)
        if tbls:
            # ë‹¨ì¼ í…Œì´ë¸”ì´ê±°ë‚˜ í—¤ë”© ë§¤í•‘ ì‹¤íŒ¨ â†’ íŒíŠ¸ ê¸°ë°˜ ê°•ì œ ë¶„ë¦¬
            base_df = _normalize_headers(tbls[0][1])
            hints = feature_hints or {}
            groups = split_single_df_feature_aware(base_df, hints)
        else:
            return (md_text, {})

    # ì›ë¬¸ ìˆœì„œ ë³´ì¡´
    ordered = []
    tbls2 = _parse_md_tables_with_heading(md_text)
    seen = set()
    for (heading, df) in tbls2:
        sheet_name, key_id = _normalize_feature_key(heading, df.iloc[0].to_dict() if len(df) else None)
        candidates = [k for k in groups.keys() if k.startswith(sheet_name)]
        name = candidates[0] if candidates else sheet_name
        if name in groups and name not in seen:
            ordered.append(name); seen.add(name)
    for name in groups.keys():
        if name not in seen:
            ordered.append(name)

    parts = []
    for name in ordered:
        df = groups[name]
        parts.append(f"## {name}")
        parts.append(_df_to_md_table(df))
        parts.append("")
    return ("\n".join(parts).strip(), groups)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [ADD] NEW: "í•¨ìˆ˜ëª… ë¶„ì„ ê¸°ë°˜" ìƒ˜í”Œ TC ìƒì„±ê¸° (ê¸°ì¡´ ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_tc_id_from_fn(fn: str, used_ids: set, seq: int | None = None) -> str:
    stop = {
        "get","set","is","has","have","do","make","build","create","update","insert","delete","remove","fetch","load","read","write",
        "put","post","patch","calc","compute","process","handle","run","exec","call","check","validate","convert","parse","format",
        "test","temp","main","init","start","stop","open","close","send","receive","retry","download","upload","save","add","sum","plus","div","divide"
    }
    s = re.sub(r"([a-z])([A-Z])", r"\1 \2", fn).replace("_"," ")
    words = [w for w in re.findall(r"[A-Za-z]+", s) if w.lower() not in stop] or re.findall(r"[A-Za-z]+", s)[:2]
    base = "".join(w.capitalize() for w in words[:3])
    base = re.sub(r"[^A-Za-z0-9]", "", base) or "Auto"
    n = 1 if seq is None else seq
    tcid = f"TC-{base}-{n:03d}"
    while tcid in used_ids:
        n += 1
        tcid = f"TC-{base}-{n:03d}"
    used_ids.add(tcid)
    return tcid

def build_function_based_sample_tc(top_functions: list[str]) -> pd.DataFrame:
    rows = []
    used_kinds = set()
    used_ids = set()

    def priority(kind: str) -> str:
        high = {"div", "auth", "write", "delete", "io", "validate"}
        return "High" if kind in high else "Medium"

    def templates_for_kind(kind: str, fn: str):
        fn_disp = fn
        if kind == "add":
            return [
                (f"{fn_disp} ì •ìƒ í•©ì‚°", "a=10, b=20 (ì •ìƒê°’)", "30 ë°˜í™˜"),
                (f"{fn_disp} í•©ì‚° ê²½ê³„ê°’", "a=-1, b=1 (ìŒìˆ˜+ì–‘ìˆ˜)", "ì˜¤ë²„í”Œë¡œìš°/ì–¸ë”í”Œë¡œìš° ì—†ì´ 0 ë°˜í™˜")
            ]
        if kind == "div":
            return [
                (f"{fn_disp} ì •ìƒ ë‚˜ëˆ—ì…ˆ", "a=6, b=3 (ì •ìƒê°’)", "2 ë°˜í™˜(ì •ìˆ˜/ì‹¤ìˆ˜ ì²˜ë¦¬ ì¼ê´€)"),
                (f"{fn_disp} 0 ë‚˜ëˆ—ì…ˆ ì˜ˆì™¸", "a=1, b=0 (ë¹„ì •ìƒ)", "ZeroDivisionError ë˜ëŠ” 400/ì˜ˆì™¸ ì½”ë“œ")
            ]
        if kind == "read":
            return [
                (f"{fn_disp} ìœ íš¨ ì¡°íšŒ", "id=1 (ì¡´ì¬)", "ì •ìƒ ë°ì´í„° ë°˜í™˜(HTTP 200/OK)"),
                (f"{fn_disp} ë¯¸ì¡´ì¬ ì¡°íšŒ", "id=999999 (ë¯¸ì¡´ì¬)", "404/ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            ]
        if kind == "write":
            return [
                (f"{fn_disp} ìœ íš¨ ì“°ê¸°", "payload={'name':'A','value':1}", "201/ì„±ê³µ ë° ì˜ì† ë°˜ì˜"),
                (f"{fn_disp} í•„ìˆ˜ê°’ ëˆ„ë½", "payload={'value':1} (name ëˆ„ë½)", "400/ê²€ì¦ ì˜¤ë¥˜ ë©”ì‹œì§€")
            ]
        if kind == "delete":
            return [
                (f"{fn_disp} ìœ íš¨ ì‚­ì œ", "id=1 (ì¡´ì¬)", "ì‚­ì œ ì„±ê³µ ë° ì¬ì¡°íšŒ ì‹œ ë¯¸ì¡´ì¬"),
                (f"{fn_disp} ì¤‘ë³µ/ë¯¸ì¡´ì¬ ì‚­ì œ", "id=999999 (ë¯¸ì¡´ì¬)", "404 ë˜ëŠ” ë©±ë“± ì²˜ë¦¬")
            ]
        if kind == "auth":
            return [
                (f"{fn_disp} ìœ íš¨ í† í° ì ‘ê·¼", "Bearer ìœ íš¨í† í°", "200/ê¶Œí•œ í—ˆìš©"),
                (f"{fn_disp} ë§Œë£Œ/ìœ„ì¡° í† í°", "Bearer ë§Œë£Œ/ìœ„ì¡° í† í°", "401/403 ì ‘ê·¼ ê±°ë¶€")
            ]
        if kind == "validate":
            return [
                (f"{fn_disp} ì´ë©”ì¼ ìœ íš¨ì„±(ì •ìƒ)", "s='user@example.com'", "True/í—ˆìš©"),
                (f"{fn_disp} ì´ë©”ì¼ ìœ íš¨ì„±(ì´ìƒ)", "s='invalid@domain'", "False/422 ë˜ëŠ” ê²€ì¦ ì‹¤íŒ¨")
            ]
        if kind == "io":
            return [
                (f"{fn_disp} ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ ì„±ê³µ", "íŒŒì¼=1MB, timeout=5s", "ì„±ê³µ/ì •ìƒ ì‘ë‹µ, ë¬´ê²°ì„± ìœ ì§€"),
                (f"{fn_disp} ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ", "timeout=1s (ì§€ì—° í™˜ê²½)", "ì¬ì‹œë„ or íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ ì²˜ë¦¬")
            ]
        return [
            (f"{fn_disp} ê¸°ë³¸ ì •ìƒ ë™ì‘", "í‘œì¤€ ì…ë ¥ 1ì„¸íŠ¸(ì •ìƒ)", "ì„±ê³µ ì½”ë“œ/ì •ìƒ ë°˜í™˜"),
            (f"{fn_disp} ë¹„ì •ìƒ ì…ë ¥ ì²˜ë¦¬", "í•„ìˆ˜ê°’ ëˆ„ë½ ë˜ëŠ” íƒ€ì… ë¶ˆì¼ì¹˜", "ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€/ì½”ë“œ ë°˜í™˜")
        ]

    def classify(fn: str) -> str:
        s = fn.lower()
        if any(k in s for k in ["add", "sum", "plus"]): return "add"
        if any(k in s for k in ["div", "divide"]): return "div"
        if any(k in s for k in ["get", "fetch", "load", "read"]): return "read"
        if any(k in s for k in ["save", "create", "update", "insert", "post", "put"]): return "write"
        if any(k in s for k in ["delete", "remove"]): return "delete"
        if any(k in s for k in ["auth", "login", "signin", "verify", "token"]): return "auth"
        if any(k in s for k in ["email", "validate", "regex", "check"]): return "validate"
        if any(k in s for k in ["upload", "download", "request", "client", "socket"]): return "io"
        return "default"

    candidates = []
    seq_counter = 1
    for fn in top_functions:
        kind = classify(fn)
        if kind in used_kinds:
            continue
        used_kinds.add(kind)
        title, inp, exp = templates_for_kind(kind, fn)[0]
        tcid = make_tc_id_from_fn(fn, used_ids, seq=seq_counter)
        seq_counter += 1
        candidates.append([kind, fn, tcid, title, inp, exp, priority(kind)])
        if len(candidates) >= 3:
            break

    result = []
    if len(candidates) >= 3:
        for c in candidates[:3]:
            kind, fn, tcid, title, inp, exp, pr = c
            result.append([tcid, title, inp, exp, pr])
    elif len(candidates) == 2:
        for c in candidates:
            kind, fn, tcid, title, inp, exp, pr = c
            result.append([tcid, title, inp, exp, pr])
    elif len(candidates) == 1:
        kind, fn, _, _, _, _, pr = candidates[0]
        t_list = templates_for_kind(kind, fn)
        for (title, inp, exp) in t_list[:2]:
            tcid = make_tc_id_from_fn(fn, used_ids, seq=seq_counter)
            seq_counter += 1
            result.append([tcid, title, inp, exp, pr])
    else:
        tcid1 = make_tc_id_from_fn("Bootstrap_Init", used_ids, seq=1)
        tcid2 = make_tc_id_from_fn("CorePath_Error", used_ids, seq=2)
        result = [
            [tcid1, "ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ê¸°ë³¸ ë¶€íŒ… ê²€ì¦", "ê¸°ë³¸ ì‹¤í–‰ í”Œë¡œìš°", "ì—ëŸ¬ ì—†ì´ ì´ˆê¸° í™”ë©´/ìƒíƒœ ë„ë‹¬", "Medium"],
            [tcid2, "í•µì‹¬ ê²½ë¡œ ì˜ˆì™¸ ì²˜ë¦¬ ê²€ì¦", "ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥(íƒ€ì… ë¶ˆì¼ì¹˜/ëˆ„ë½)", "ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€/ì½”ë“œ ë°˜í™˜", "High"],
        ]
    return pd.DataFrame(result, columns=["TC ID","ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼","ìš°ì„ ìˆœìœ„"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [ADD] ë™ì  ì„¤ëª… ìƒì„±ì„ ìœ„í•œ ìœ í‹¸ (ìš°ì„ ìˆœìœ„ ì •ê·œí™”/ì¶”ë¡  í¬í•¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [ADD] ìš°ì„ ìˆœìœ„ í† í° ì •ê·œí™”
def _normalize_priority_token(v: str) -> str:
    s = str(v or "").strip().lower()
    if not s:
        return ""
    mapping = {
        "1": "High", "h": "High", "high": "High", "ë†’ìŒ": "High", "ìƒ": "High", "í•„ìˆ˜": "High", "critical": "High", "crit": "High",
        "2": "Medium", "m": "Medium", "med": "Medium", "medium": "Medium", "ì¤‘ê°„": "Medium", "ë³´í†µ": "Medium", "ì¤‘": "Medium",
        "3": "Low", "l": "Low", "low": "Low", "ë‚®ìŒ": "Low", "í•˜": "Low", "optional": "Low", "ì˜µì…˜": "Low"
    }
    return mapping.get(s, "High" if "high" in s else ("Medium" if "med" in s else ("Low" if "low" in s else "")))

# [ADD] í–‰ ë‹¨ìœ„ ìš°ì„ ìˆœìœ„ ì¶”ë¡ 
def _infer_priority_from_text(text: str) -> str:
    s = (text or "").lower()
    if any(k in s for k in ["zerodivision", "division by zero", "0ìœ¼ë¡œ", "error", "exception", "fatal", "ê¶Œí•œ", "unauthorized", "forbidden", "not found", "401", "403", "404", "timeout", "íƒ€ì„ì•„ì›ƒ", "invalid", "ì˜¤ë¥˜"]):
        return "High"
    if any(k in s for k in ["ê²½ê³„", "boundary", "edge", "ìµœëŒ€", "ìµœì†Œ", "ìŒìˆ˜", "ì†Œìˆ˜", "float", "overflow", "underflow"]):
        return "Medium"
    return "Medium"

# [ADD] DF ì „ì²´ì— ëŒ€í•´ ìš°ì„ ìˆœìœ„ ì •ê·œí™”+ì¶”ë¡  ë°˜ì˜
def _ensure_priorities(df: pd.DataFrame) -> pd.DataFrame:
    df2 = df.copy()
    if "ìš°ì„ ìˆœìœ„" not in df2.columns:
        df2["ìš°ì„ ìˆœìœ„"] = ""
    norm_vals = []
    for _, row in df2.iterrows():
        raw = str(row.get("ìš°ì„ ìˆœìœ„", "")).strip()
        merged_text = " ".join([str(row.get(c, "")) for c in ["ê¸°ëŠ¥ ì„¤ëª…", "ì…ë ¥ê°’", "ì˜ˆìƒ ê²°ê³¼"]])
        norm = _normalize_priority_token(raw)
        if not norm:
            norm = _infer_priority_from_text(merged_text)
        norm_vals.append(norm)
    df2["ìš°ì„ ìˆœìœ„"] = norm_vals
    return df2

# [FIX] ìš°ì„ ìˆœìœ„ ë¶„í¬ ê³„ì‚°: ì •ê·œí™”/ì¶”ë¡  ì´í›„ ì¹´ìš´íŠ¸
def _priority_counts(df: pd.DataFrame) -> dict:
    df2 = _ensure_priorities(df)
    vals = df2["ìš°ì„ ìˆœìœ„"].astype(str).str.strip().str.title().tolist()
    c = Counter(vals)
    return {"High": c.get("High", 0), "Medium": c.get("Medium", 0), "Low": c.get("Low", 0)}

# [ADD] ê¸°ëŠ¥ëª… â†’ í•œêµ­ì–´ ì„¤ëª… íœ´ë¦¬ìŠ¤í‹±
def _feature_korean_desc(name: str, fallback_from_rows: str = "") -> str:
    n = (name or "").lower()
    if any(k in n for k in ["add", "sum", "plus"]):
        return "ë‘ ê°œì˜ ìˆ˜ë¥¼ ë”í•´ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
    if any(k in n for k in ["div", "divide"]):
        return "ë‘ ìˆ˜ë¥¼ ë‚˜ëˆ„ì–´ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë©°, 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²½ìš° ì˜ˆì™¸ê°€ ë°œìƒí•©ë‹ˆë‹¤."
    if any(k in n for k in ["email", "isemail", "validator", "validate"]):
        return "ë¬¸ìì—´ì´ ìœ íš¨í•œ ì´ë©”ì¼ í˜•ì‹ì¸ì§€ ê²€ì‚¬í•©ë‹ˆë‹¤."
    if "health" in n:
        return "í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ì˜ ê°€ìš©ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤."
    # [ADD] ì ì ˆí•œ íœ´ë¦¬ìŠ¤í‹±ì´ ì—†ì„ ê²½ìš°, ì²« í–‰ì˜ ê¸°ëŠ¥ ì„¤ëª…ì„ ì§§ê²Œ ì°¨ìš©
    fb = fallback_from_rows.strip()
    return fb if fb else f"â€˜{name}â€™ ê¸°ëŠ¥ì˜ í•µì‹¬ ë™ì‘ì„ ê²€ì¦í•©ë‹ˆë‹¤."

def _extract_endpoints(text: str) -> list[str]:
    eps = set(re.findall(r"/[A-Za-z0-9_\-./]+", text))
    cleaned = sorted({e.strip().rstrip(".,)") for e in eps if len(e) <= 64})
    return cleaned[:5]

def _classify_scenario_bucket(s: str) -> str:
    s = s.lower()
    if any(k in s for k in ["ì˜¤ë¥˜", "error", "ì˜ˆì™¸", "invalid", "0ìœ¼ë¡œ", "zero", "null", "timeout", "ê¶Œí•œ", "401", "403", "404"]):
        return "ì˜ˆì™¸"
    if any(k in s for k in ["ê²½ê³„", "boundary", "ìµœëŒ€", "ìµœì†Œ", "ìŒìˆ˜", "ì†Œìˆ˜", "edge", "limit"]):
        return "ê²½ê³„"
    return "ì •ìƒ"

# [FIX] ì‹¤ì œë¡œ í™”ë©´ì— ë„£ì„ ë™ì  ì„¤ëª… ë§ˆí¬ë‹¤ìš´ ìƒì„±
#      (ìš”ì²­ ë°˜ì˜) ì¶œë ¥ êµ¬ì„±: ê¸°ëŠ¥ì„¤ëª…, ìš°ì„ ìˆœìœ„ ë¶„í¬, ìš”ì•½  â€• ê·¸ë¦¬ê³  í—¤ë”ëŠ” "Feature (ì´ Nê±´)" í˜•íƒœ
def build_dynamic_explanations(groups: dict[str, pd.DataFrame]) -> str:
    if not groups:
        return "_ì„¤ëª…ì„ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤._"

    parts = []
    for feature_name, df in groups.items():
        df_norm = _ensure_priorities(df)

        # ê¸°ëŠ¥ ì„¤ëª…: íœ´ë¦¬ìŠ¤í‹± + ì²« í–‰ ë³´ì¡°
        first_desc = str(df_norm.iloc[0]["ê¸°ëŠ¥ ì„¤ëª…"]) if len(df_norm) else ""
        func_desc = _feature_korean_desc(feature_name, fallback_from_rows=first_desc)

        # ìš°ì„ ìˆœìœ„ ë¶„í¬
        pr = _priority_counts(df_norm)

        # ì‹œë‚˜ë¦¬ì˜¤ ì„±í–¥(ì •ìƒ/ì˜ˆì™¸/ê²½ê³„ ë¹„ìœ¨)ë¡œ ìš”ì•½ ë¬¸êµ¬ ê°€ë³€í™”
        buckets = Counter()
        for _, row in df_norm.iterrows():
            s = " ".join([str(row.get(c,"")) for c in ["ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼"]])
            buckets[_classify_scenario_bucket(s)] += 1

        endpoints = _extract_endpoints(" ".join(df_norm[["ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼"]].astype(str).fillna("").values.ravel().tolist()))
        total = len(df_norm)

        # [FIX] í—¤ë” í¬ë§· ë³€ê²½: "Div (ì´ 3ê±´)" í˜•íƒœ (tc ë²”ìœ„ ì œê±°)
        parts.append(f"#### {feature_name} (ì´ {total}ê±´)")
        parts.append(f"- **ê¸°ëŠ¥ ì„¤ëª…**: {func_desc}")
        parts.append(f"- **ìš°ì„ ìˆœìœ„ ë¶„í¬**: High {pr['High']} Â· Medium {pr['Medium']} Â· Low {pr['Low']}")

        # [FIX] ìš°ì„ ìˆœìœ„ ì •ë¦¬ ì„¹ì…˜ ì‚­ì œ (ìš”ì²­ ë°˜ì˜)

        # [FIX] ìš”ì•½(ê¸°ëŠ¥ë³„ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥´ê²Œ)
        summary_bits = []
        if buckets.get("ì˜ˆì™¸", 0) > 0:
            summary_bits.append("ì˜ˆì™¸ ì²˜ë¦¬ë¡œ ì•ˆì •ì„± ê²€ì¦ì„ ê°•í™”")
        if buckets.get("ê²½ê³„", 0) > 0:
            summary_bits.append("ê²½ê³„ ì…ë ¥ì„ í¬í•¨í•´ ê²¬ê³ ì„± í™•ì¸")
        if endpoints:
            summary_bits.append("ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ ë™ì‘ ì¼ê´€ì„± í™•ì¸")
        if not summary_bits:
            summary_bits.append("ì •ìƒÂ·ê²½ê³„ ìƒí™©ì„ ê· í˜• ìˆê²Œ ê²€ì¦")

        parts.append(f"- **ìš”ì•½**: {', '.join(summary_bits)}.")
        parts.append("")  # spacing

    return "\n".join(parts).strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§ª TAB 1: ì†ŒìŠ¤ì½”ë“œ â†’ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìë™ ìƒì„±ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with code_tab:
    st.subheader("ğŸ§ª ì†ŒìŠ¤ì½”ë“œ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìë™ ìƒì„±ê¸°")

    # (ìœ ì§€) ìƒ˜í”Œ ì½”ë“œ ZIPë§Œ ì œê³µ
    st.download_button(
        "â¬‡ï¸ ìƒ˜í”Œ ì½”ë“œ ZIP ë‹¤ìš´ë¡œë“œ",
        data=build_sample_code_zip(),
        file_name="sample_code.zip",
        help="ê°„ë‹¨í•œ Python í•¨ìˆ˜/ê²€ì¦ ë¡œì§ 3íŒŒì¼ í¬í•¨"
    )

    uploaded_file = st.file_uploader("ğŸ“‚ ì†ŒìŠ¤ì½”ë“œ zip íŒŒì¼ ì—…ë¡œë“œ", type=["zip"], key="code_zip")

    def need_llm_call(uploaded_file, model, role):
        return uploaded_file and (st.session_state.last_uploaded_file != uploaded_file.name or st.session_state.last_model != model or st.session_state.last_role != role)

    qa_role = st.session_state.get("qa_role", "ê¸°ëŠ¥ QA")

    # Auto-Preview(ìš”ì•½) & Sample TC (ê¸°ì¡´ ìœ ì§€) + [ADD] ê¸°ëŠ¥íŒíŠ¸ ìƒì„±
    code_bytes = None
    stats = {"total_files":0,"lang_counts":Counter(),"top_functions":[]}
    if uploaded_file:
        code_bytes = uploaded_file.getvalue()
        stats = analyze_code_zip(code_bytes)
        # [ADD] ê¸°ëŠ¥ íŒíŠ¸ ì €ì¥ (í›„ì²˜ë¦¬ ê°•ì œ ë¶„ë¦¬ìš©)
        st.session_state.feature_hints = build_feature_hints(stats)

        with st.expander("ğŸ“Š Auto-Preview(ìš”ì•½)", expanded=True):
            lang_str = ", ".join([f"{k} {v}ê°œ" for k, v in stats["lang_counts"].most_common()]) if stats["lang_counts"] else "ê°ì§€ëœ ì–¸ì–´ ì—†ìŒ"
            funcs_cnt = len(stats["top_functions"])
            expected_tc = estimate_tc_count(stats)
            st.markdown(
                f"- **íŒŒì¼ ìˆ˜**: {stats['total_files']}\n"
                f"- **ì–¸ì–´ ë¶„í¬**: {lang_str}\n"
                f"- **í•¨ìˆ˜/ì—”ë“œí¬ì¸íŠ¸ ìˆ˜(ì¶”ì •)**: {funcs_cnt}\n"
                f"- **ì˜ˆìƒ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ê°œìˆ˜(ì¶”ì •)**: {expected_tc}"
            )

        with st.expander("ğŸ”® Auto-Preview(Sample TC)", expanded=True):
            sample_df = build_function_based_sample_tc(stats.get("top_functions", []))
            st.dataframe(sample_df, use_container_width=True)

    # LLM í˜¸ì¶œ
    if uploaded_file and need_llm_call(uploaded_file, model, qa_role):
        st.session_state["is_loading"] = True
        with st.spinner("ğŸ” LLM í˜¸ì¶œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."):
            with tempfile.TemporaryDirectory() as tmpdir:
                zip_path = os.path.join(tmpdir, uploaded_file.name)
                with open(zip_path, "wb") as f:
                    f.write(code_bytes if code_bytes is not None else uploaded_file.read())
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(tmpdir)

                full_code = ""
                for root, _, files in os.walk(tmpdir):
                    for file in files:
                        if file.endswith((".py", ".java", ".js", ".ts", ".cpp", ".c", ".cs")):
                            file_path = os.path.join(root, file)
                            try:
                                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                                    code = f.read()
                                full_code += f"\n\n# FILE: {file}\n{code}"
                            except:
                                continue

                # [FIX] í”„ë¡¬í”„íŠ¸ ë³´ê°• (í‘œë§Œ ìƒì„±í•˜ë„ë¡ ìœ ë„)
                feature_hints = st.session_state.get("feature_hints") or {}
                hint_blocks = []
                for key, toks in feature_hints.items():
                    disp = _display_from_key(key)
                    toks_view = ", ".join(sorted(set(toks))[:6])
                    hint_blocks.append(f"- {disp} (key={key}) : {toks_view}")
                hints_md = "\n".join(hint_blocks) if hint_blocks else "- General (key=general)"

                prompt = f"""
ë„ˆëŠ” ì‹œë‹ˆì–´ QA ì—”ì§€ë‹ˆì–´ì´ë©°, í˜„ì¬ '{qa_role}' ì—­í• ì„ ë§¡ê³  ìˆë‹¤. ì•„ë˜ ì†ŒìŠ¤ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ **ê¸°ëŠ¥ë³„ ì„¹ì…˜**ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ì‘ì„±í•˜ë¼. ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì§€ì¼œë¼:
- ê° ê¸°ëŠ¥ì€ "## ê¸°ëŠ¥ëª…" í—¤ë”©ìœ¼ë¡œ ì‹œì‘í•œë‹¤. (ì˜ˆ: ## AlarmManager)
- ê° ê¸°ëŠ¥ ì„¹ì…˜ë§ˆë‹¤ **í•˜ë‚˜ì˜ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”**ë§Œ í¬í•¨í•œë‹¤.
- í…Œì´ë¸” ì»¬ëŸ¼: | TC ID | ê¸°ëŠ¥ ì„¤ëª… | ì…ë ¥ê°’ | ì˜ˆìƒ ê²°ê³¼ | ìš°ì„ ìˆœìœ„ |
- **TC IDëŠ” ë°˜ë“œì‹œ tc-<feature-key>-NNN í˜•ì‹**ì„ ì‚¬ìš©í•˜ë¼. (ì˜ˆ: tc-alarm-001)
- <feature-key>ëŠ” ì•„ë˜ íŒíŠ¸ ëª©ë¡ì˜ key ì¤‘ ê°€ì¥ ì í•©í•œ ê°’ì„ ì‚¬ìš©í•œë‹¤.
- ê° ê¸°ëŠ¥ ì„¹ì…˜ë§ˆë‹¤ NNNì€ 001ë¶€í„° ë‹¤ì‹œ ì‹œì‘í•œë‹¤.
- ê° ê¸°ëŠ¥ ì„¹ì…˜ ì•„ë˜ì— ìƒì„±ëœ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ì— ê´€í•œ ì„¤ëª…ì„ ì‘ì„±í•œë‹¤.

[ê¸°ëŠ¥ íŒíŠ¸ ëª©ë¡]
{hints_md}

[ì†ŒìŠ¤ì½”ë“œ]
{full_code}
                """

                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={"Authorization": f"Bearer {API_KEY}"},
                    json={
                        "model": model,
                        "messages": [{
                            "role": "user",
                            "content": prompt
                        }]
                    }
                )
                result = response.json()["choices"][0]["message"]["content"]
                st.session_state.llm_result = result

                # [FIX] ê²°ê³¼ëŠ” 'LLM ì›ë¬¸(ì •ê·œí™”)'ë§Œ í‘œì‹œ: íŒíŠ¸ ê¸°ë°˜ ê°•ì œ ë¶„ë¦¬/ì •ê·œí™” í¬í•¨
                try:
                    normalized_md, groups = rebuild_normalized_markdown(result, st.session_state.get("feature_hints"))
                    st.session_state.normalized_markdown = normalized_md
                    st.session_state.parsed_groups = groups if groups else None
                    st.session_state.parsed_df = concat_groups_for_view(groups) if groups else None
                except Exception:
                    st.session_state.normalized_markdown = result
                    st.session_state.parsed_groups = None
                    st.session_state.parsed_df = None

                st.session_state.last_uploaded_file = uploaded_file.name
                st.session_state.last_model = model
                st.session_state.last_role = qa_role
                st.session_state["is_loading"] = False

    # [FIX] ê²°ê³¼ í‘œì‹œ: í—¤ë” ë¬¸êµ¬ + ë™ì  ì„¤ëª… ì„¹ì…˜(ìš”ì²­ ë°˜ì˜: ê¸°ëŠ¥ì„¤ëª…/ìš°ì„ ìˆœìœ„ ë¶„í¬/ìš”ì•½ë§Œ)
    if st.session_state.llm_result:
        st.success("âœ… í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
        # (ìš”ì²­1) ë¬¸êµ¬ ë³€ê²½
        st.markdown("## ğŸ“‹ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤")
        # (ìš”ì²­2) ì‘ì€ ê¸€ì”¨, ê²€ì •ìƒ‰ ìº¡ì…˜ ì¶”ê°€
        st.markdown(
            '<small style="color:#000">'
            'ì•„ë˜ëŠ” ì œê³µëœ ì†ŒìŠ¤ì½”ë“œë¥¼ ë¶„ì„í•œ í›„, ê¸°ëŠ¥ ë‹¨ìœ„ì˜ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì…ë‹ˆë‹¤. '
            'ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ëŠ” ê¸°ëŠ¥ ì„¤ëª…, ì…ë ¥ê°’, ì˜ˆìƒ ê²°ê³¼, ê·¸ë¦¬ê³  ìš°ì„ ìˆœìœ„ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.'
            '</small>',
            unsafe_allow_html=True
        )
        # [REQ] LLM ì›ë¬¸ ì¶œë ¥: ì •ê·œí™”ë³¸ ëŒ€ì‹  LLM ê²°ê³¼ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ í‘œì‹œí•˜ì—¬,
        # ê³¼ê±° ì†ŒìŠ¤ì²˜ëŸ¼ ìƒí™©ì— ë”°ë¼ ì„¤ëª…ì´ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ë  ìˆ˜ ìˆë„ë¡ í•¨
        st.markdown(st.session_state.llm_result)

        # [REQ] ì„¤ëª… í•˜ë“œì½”ë”© ì¶œë ¥ ì œê±°: ê³ ì • "ì„¤ëª…" ì„¹ì…˜ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬
        # í…Œì´ë¸” ì•„ë˜ì— ê³ ì • ë¬¸êµ¬ê°€ í•­ìƒ ë¶™ëŠ” ë™ì‘ì„ ë¹„í™œì„±í™”

    # [FIX] (ìš”ì²­4) ë¬´ìŠ¨ ì¼ì´ ìˆì–´ë„ 'ì—‘ì…€ ë‹¤ìš´ë¡œë“œ' ë²„íŠ¼ì€ í•­ìƒ í‘œì‹œ
    excel_bytes = None
    try:
        bio = io.BytesIO()
        if st.session_state.get("parsed_groups"):
            with pd.ExcelWriter(bio, engine="openpyxl") as writer:
                for key, df in st.session_state.parsed_groups.items():
                    # ì €ì¥ ì „ ìš°ì„ ìˆœìœ„ ì •ê·œí™”ë¡œ ì¼ê´€ì„± ê°œì„ 
                    df_out = _ensure_priorities(df)
                    sheet = re.sub(r"[^A-Za-z0-9ê°€-í£_ -]", "", key)[:31] or "General"
                    df_out.to_excel(writer, index=False, sheet_name=sheet)
            excel_bytes = bio.getvalue()
        elif st.session_state.get("parsed_df") is not None:
            with pd.ExcelWriter(bio, engine="openpyxl") as writer:
                pd.DataFrame(_ensure_priorities(st.session_state.parsed_df)).to_excel(writer, index=False, sheet_name="í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤")
            excel_bytes = bio.getvalue()
        elif st.session_state.get("normalized_markdown") or st.session_state.get("llm_result"):
            md = st.session_state.get("normalized_markdown") or st.session_state.get("llm_result") or ""
            groups = group_tables_and_renumber(md)
            with pd.ExcelWriter(bio, engine="openpyxl") as writer:
                if groups:
                    for key, df in groups.items():
                        df_out = _ensure_priorities(df)
                        sheet = re.sub(r"[^A-Za-z0-9ê°€-í£_ -]", "", key)[:31] or "General"
                        df_out.to_excel(writer, index=False, sheet_name=sheet)
                else:
                    pd.DataFrame(columns=["TC ID","ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼","ìš°ì„ ìˆœìœ„"]).to_excel(
                        writer, index=False, sheet_name="í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤"
                    )
            excel_bytes = bio.getvalue()
        else:
            with pd.ExcelWriter(bio, engine="openpyxl") as writer:
                pd.DataFrame(columns=["TC ID","ê¸°ëŠ¥ ì„¤ëª…","ì…ë ¥ê°’","ì˜ˆìƒ ê²°ê³¼","ìš°ì„ ìˆœìœ„"]).to_excel(
                    writer, index=False, sheet_name="í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤"
                )
            excel_bytes = bio.getvalue()
    except Exception:
        excel_bytes = build_sample_tc_excel()

    st.download_button("â¬‡ï¸ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", data=excel_bytes, file_name="í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤.xlsx")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“‘ TAB 2: í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ â†’ ëª…ì„¸ì„œ ìš”ì•½ (ê¸°ì¡´ ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tc_tab:
    st.subheader("ğŸ“‘ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ê¸°ë°˜ ê¸°ëŠ¥/ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ì¶”ì¶œê¸°")
    st.download_button(
        "â¬‡ï¸ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
        data=build_sample_tc_excel(),
        file_name="í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤_ìƒ˜í”Œ.xlsx",
        help="í•„ìˆ˜ ì»¬ëŸ¼( TC ID, ê¸°ëŠ¥ ì„¤ëª…, ì…ë ¥ê°’, ì˜ˆìƒ ê²°ê³¼, ìš°ì„ ìˆœìœ„ ) í¬í•¨"
    )

    tc_file = st.file_uploader("ğŸ“‚ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ íŒŒì¼ ì—…ë¡œë“œ (.xlsx, .csv)", type=["xlsx", "csv"], key="tc_file")
    summary_type = st.selectbox("ğŸ“Œ ìš”ì•½ ìœ í˜•", ["ê¸°ëŠ¥ ëª…ì„¸ì„œ", "ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œ"], key="summary_type")

    if st.button("ğŸš€ ëª…ì„¸ì„œ ìƒì„±í•˜ê¸°", disabled=st.session_state["is_loading"]) and tc_file:
        st.session_state["is_loading"] = True
        try:
            # [REQ] ì—¬ëŸ¬ ì‹œíŠ¸ ì§€ì›: XLSXëŠ” ëª¨ë“  ì‹œíŠ¸ë¥¼ ì½ì–´ ì„¸ë¡œ ê²°í•©
            if tc_file.name.endswith("csv"):
                df = pd.read_csv(tc_file)
            else:
                sheets_dict = pd.read_excel(tc_file, sheet_name=None)  # ëª¨ë“  ì‹œíŠ¸ ë¡œë“œ
                frames = []
                for sheet_name, sdf in sheets_dict.items():
                    if isinstance(sdf, pd.DataFrame) and not sdf.empty:
                        frames.append(sdf)
                df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
                
        except Exception as e:
            st.session_state["is_loading"] = False
            st.error(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            st.stop()

        with st.spinner("ğŸ” LLM í˜¸ì¶œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."):
            required_cols = ["TC ID", "ê¸°ëŠ¥ ì„¤ëª…", "ì…ë ¥ê°’", "ì˜ˆìƒ ê²°ê³¼"]
            if not all(col in df.columns for col in required_cols):
                st.session_state["is_loading"] = False
                st.warning("âš ï¸ ë‹¤ìŒ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤: TC ID, ê¸°ëŠ¥ ì„¤ëª…, ì…ë ¥ê°’, ì˜ˆìƒ ê²°ê³¼")
                st.stop()

            prompt = f"""
ë„ˆëŠ” í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ê·¸ ê¸°ë°˜ì´ ë˜ëŠ” {summary_type}ë¥¼ ì‘ì„±í•˜ëŠ” QA ì „ë¬¸ê°€ì´ë‹¤.
í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ íŒŒì¼ì´ ì—¬ëŸ¬ ì‹œíŠ¸ë¡œ êµ¬ì„±ëœ ê²½ìš°, **ëª¨ë“  ì‹œíŠ¸ì˜ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬** ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•˜ë¼.

í˜•ì‹:
- ê¸°ëŠ¥ëª… ë˜ëŠ” ìš”êµ¬ì‚¬í•­ ì œëª©
- ì„¤ëª…
- ê¸°ëŒ€ íš¨ê³¼

í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ëª©ë¡:
{df.to_csv(index=False)}
            """

            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={"Authorization": f"Bearer {API_KEY}"},
                json={
                    "model": model,
                    "messages": [{
                        "role": "user",
                        "content": prompt
                    }]
                }
            )
            if response.status_code == 200:
                result = response.json()["choices"][0]["message"]["content"]
                st.session_state.spec_result = result
            else:
                st.error("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨")
                st.text(response.text)

            st.session_state["is_loading"] = False

    if st.session_state.spec_result:
        st.success("âœ… ëª…ì„¸ì„œ ìƒì„± ì™„ë£Œ!")
        st.markdown("## ğŸ“‹ ìë™ ìƒì„±ëœ ëª…ì„¸ì„œ")
        st.markdown(st.session_state.spec_result)
        st.download_button("â¬‡ï¸ ëª…ì„¸ì„œ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ", data=st.session_state.spec_result, file_name="ê¸°ëŠ¥_ìš”êµ¬ì‚¬í•­_ëª…ì„¸ì„œ.txt")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ TAB 3: ì—ëŸ¬ ë¡œê·¸ â†’ ì¬í˜„ ì‹œë‚˜ë¦¬ì˜¤ (ê¸°ì¡´ ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with log_tab:
    st.subheader("ğŸ ì—ëŸ¬ ë¡œê·¸ ê¸°ë°˜ ì¬í˜„ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ê¸°")

    sample_log = """[InstallShield Silent]
Version=v7.00
File=Log File

[ResponseResult]
ResultCode=0

[Application]
Name=Realtek Audio Driver
Version=4.92
Company=Realtek Semiconductor Corp.
Lang=0412
"""
    st.download_button(
        "â¬‡ï¸ ìƒ˜í”Œ ì—ëŸ¬ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ",
        data=sample_log,
        file_name="sample_error_log.log",
        disabled=st.session_state["is_loading"]
    )

    log_file = st.file_uploader("ğŸ“‚ ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ì—…ë¡œë“œ (.log, .txt)", type=["log", "txt"], key="log_file")

    if not API_KEY:
        st.warning("ğŸ” OpenRouter API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    raw_log_cache = None
    if log_file:
        raw_log_cache = log_file.read().decode("utf-8", errors="ignore")

    if st.button("ğŸš€ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±í•˜ê¸°", disabled=st.session_state["is_loading"]) and raw_log_cache:
        st.session_state["is_loading"] = True
        with st.spinner("LLMì„ í˜¸ì¶œ ì¤‘ì…ë‹ˆë‹¤..."):
            qa_role = st.session_state.get("qa_role", "ê¸°ëŠ¥ QA")
            chosen_model = model
            budget = safe_char_budget(chosen_model, token_margin=1024)

            focused_log, stats = preprocess_log_text(
                raw_log_cache, context_lines=5, keep_last_lines_if_empty=2000, char_budget=budget)
            st.info(
                f"ì „ì²˜ë¦¬ ê²°ê³¼: ë¬¸ì {stats['kept_chars']:,}/{stats['char_budget']:,} ì‚¬ìš© (ì „ì²´ ë¼ì¸ {stats['total_lines']:,})."
            )
            st.markdown("**ì „ì²˜ë¦¬ ìŠ¤ë‹ˆí« (ìƒìœ„ 120ì¤„):**")
            st.code("\n".join(focused_log.splitlines()[:120]), language="text")

            prompt = f"""ë„ˆëŠ” ì‹œë‹ˆì–´ QA ì—”ì§€ë‹ˆì–´ì´ë©°, í˜„ì¬ '{qa_role}' ì—­í• ì„ ë§¡ê³  ìˆë‹¤.
ì•„ë˜ ìš”ì•½Â·ë°œì·Œí•œ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ í•´ë‹¹ ì˜¤ë¥˜ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆëŠ” í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‘ì„±í•˜ë¼.

ì‹œë‚˜ë¦¬ì˜¤ í˜•ì‹:
1. ì‹œë‚˜ë¦¬ì˜¤ ì œëª©:
2. ì „ì œ ì¡°ê±´:
3. í…ŒìŠ¤íŠ¸ ì…ë ¥ê°’:
4. ì¬í˜„ ì ˆì°¨:
5. ê¸°ëŒ€ ê²°ê³¼:

ì „ì²˜ë¦¬ëœ ì—ëŸ¬ ë¡œê·¸:
{focused_log}
"""

            try:
                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={"Authorization": f"Bearer {API_KEY}"},
                    json={
                        "model": chosen_model,
                        "messages": [{
                            "role": "user",
                            "content": prompt
                        }],
                        "temperature": 0.2
                    },
                    timeout=120,
                )
                if response.status_code == 200:
                    content = response.json()["choices"][0]["message"]["content"]
                    st.session_state.scenario_result = content
                else:
                    st.error("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨")
                    st.caption("ì„œë²„ ì‘ë‹µ:")
                    st.text(response.text)
            except requests.exceptions.RequestException as e:
                st.error("âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ")
                st.exception(e)

            st.session_state["is_loading"] = False

    if st.session_state.scenario_result:
        st.success("âœ… ì¬í˜„ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì™„ë£Œ!")
        st.markdown("## ğŸ“‹ ìë™ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤")
        st.markdown(st.session_state.scenario_result)
        st.download_button("â¬‡ï¸ ì‹œë‚˜ë¦¬ì˜¤ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ", data=st.session_state.scenario_result, file_name="ì¬í˜„_ì‹œë‚˜ë¦¬ì˜¤.txt")
